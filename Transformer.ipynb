{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import pickle as pk\n",
    "\n",
    "from tqdm import tqdm\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_idx = 0\n",
    "SOS_idx = 1\n",
    "EOS_idx = 2\n",
    "UNK_idx= 3\n",
    "batch_size = 64\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "\n",
    "LOG = \"Trial_Dec14_2\"\n",
    "EPOCH = 50\n",
    "label_smoothing = True\n",
    "SAVE_MODEL = 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language(object):\n",
    "    def __init__(self, name, i2t, t2i, embedding_matrix, train, test, val):\n",
    "        self.name = name\n",
    "        self.idx2token = i2t\n",
    "        self.token2idx = t2i\n",
    "        self.embedding_mat = embedding_matrix\n",
    "        self.train_idx = train\n",
    "        self.test_idx = test\n",
    "        self.val_idx = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pk.load(open('../data/vi1.1w-en6k.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(sent, max_len):\n",
    "    if len(sent) > max_len-2:\n",
    "        return [SOS_idx]+sent[:max_len-2]+[EOS_idx]\n",
    "    else:\n",
    "        return [SOS_idx]+sent+[EOS_idx] + [PAD_idx]*(max_len-2-len(sent))\n",
    "\n",
    "def paired_collate_fn(insts):\n",
    "    src_insts, tgt_insts = list(zip(*insts))\n",
    "    src_insts = collate_fn(src_insts)\n",
    "    tgt_insts = collate_fn(tgt_insts)\n",
    "    return (*src_insts, *tgt_insts)\n",
    "\n",
    "def collate_fn(insts):\n",
    "    ''' Pad the instance to the max seq length in batch '''\n",
    "\n",
    "    max_len = MAX_SENTENCE_LENGTH\n",
    "    batch_seq = np.array([parser(inst, max_len) for inst in insts])\n",
    "    batch_pos = np.array([\n",
    "        [pos_i+1 if w_i != PAD_idx else 0\n",
    "         for pos_i, w_i in enumerate(inst)] for inst in batch_seq])\n",
    "\n",
    "    batch_seq = torch.LongTensor(batch_seq)\n",
    "    batch_pos = torch.LongTensor(batch_pos)\n",
    "\n",
    "    return batch_seq, batch_pos\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, src_word2idx, tgt_word2idx,\n",
    "        src_insts=None, tgt_insts=None):\n",
    "\n",
    "        assert src_insts\n",
    "        assert not tgt_insts or (len(src_insts) == len(tgt_insts))\n",
    "\n",
    "        src_idx2word = {idx:word for word, idx in src_word2idx.items()}\n",
    "        self._src_word2idx = src_word2idx\n",
    "        self._src_idx2word = src_idx2word\n",
    "        self._src_insts = src_insts\n",
    "\n",
    "        tgt_idx2word = {idx:word for word, idx in tgt_word2idx.items()}\n",
    "        self._tgt_word2idx = tgt_word2idx\n",
    "        self._tgt_idx2word = tgt_idx2word\n",
    "        self._tgt_insts = tgt_insts\n",
    "\n",
    "    @property\n",
    "    def n_insts(self):\n",
    "        ''' Property for dataset size '''\n",
    "        return len(self._src_insts)\n",
    "\n",
    "    @property\n",
    "    def src_vocab_size(self):\n",
    "        ''' Property for vocab size '''\n",
    "        return len(self._src_word2idx)\n",
    "\n",
    "    @property\n",
    "    def tgt_vocab_size(self):\n",
    "        ''' Property for vocab size '''\n",
    "        return len(self._tgt_word2idx)\n",
    "\n",
    "    @property\n",
    "    def src_word2idx(self):\n",
    "        ''' Property for word dictionary '''\n",
    "        return self._src_word2idx\n",
    "\n",
    "    @property\n",
    "    def tgt_word2idx(self):\n",
    "        ''' Property for word dictionary '''\n",
    "        return self._tgt_word2idx\n",
    "\n",
    "    @property\n",
    "    def src_idx2word(self):\n",
    "        ''' Property for index dictionary '''\n",
    "        return self._src_idx2word\n",
    "\n",
    "    @property\n",
    "    def tgt_idx2word(self):\n",
    "        ''' Property for index dictionary '''\n",
    "        return self._tgt_idx2word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_insts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self._tgt_insts:\n",
    "            return self._src_insts[idx], self._tgt_insts[idx]\n",
    "        return self._src_insts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lan = {\"name\": data['src'].name,\n",
    "              \"token2id\": data['src'].token2idx,\n",
    "              \"id2token\": data['src'].idx2token,\n",
    "              \"train_instance\": data['src'].train_idx,\n",
    "              \"val_instance\": data['src'].val_idx,\n",
    "              \"test_instance\": data['src'].test_idx\n",
    "             }\n",
    "\n",
    "target_lan = {\n",
    "              \"name\": data['tgt'].name,\n",
    "              \"token2id\": data['tgt'].token2idx,\n",
    "              \"id2token\": data['tgt'].idx2token,\n",
    "              \"train_instance\": data['tgt'].train_idx,\n",
    "              \"val_instance\": data['tgt'].val_idx,\n",
    "              \"test_instance\": data['tgt'].test_idx\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        TranslationDataset(\n",
    "            src_word2idx=data['src'].token2idx,\n",
    "            tgt_word2idx=data['tgt'].token2idx,\n",
    "            src_insts=data['src'].train_idx,\n",
    "            tgt_insts=data['tgt'].train_idx),\n",
    "        num_workers=2,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        TranslationDataset(\n",
    "            src_word2idx=data['src'].token2idx,\n",
    "            tgt_word2idx=data['tgt'].token2idx,\n",
    "            src_insts=data['src'].val_idx,\n",
    "            tgt_insts=data['tgt'].val_idx),\n",
    "        num_workers=2,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=paired_collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        TranslationDataset(\n",
    "            src_word2idx=data['src'].token2idx,\n",
    "            tgt_word2idx=data['tgt'].token2idx,\n",
    "            src_insts=data['src'].test_idx,\n",
    "            tgt_insts=data['tgt'].test_idx),\n",
    "        num_workers=2,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Vietnam, vocab_size: 11004 \n",
      "Target: English, vocab_size: 6604\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = train_loader.dataset.src_vocab_size\n",
    "tgt_vocab_size = train_loader.dataset.tgt_vocab_size\n",
    "print (\"Source: {}, vocab_size: {} \\nTarget: {}, vocab_size: {}\"\n",
    "               .format(source_lan['name'], src_vocab_size, target_lan['name'], tgt_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function perform dot product attention, similar to the one in the dot-product attention model\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, sqrt_dim, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sqrt_dim = sqrt_dim\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        attn = torch.bmm(query, key.transpose(1, 2))\n",
    "        attn = attn / self.sqrt_dim\n",
    "#         Mask out the padded token\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "#         Convert to probability\n",
    "        attn = self.softmax(attn)\n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "        attn = self.dropout(attn)\n",
    "#         Apply the attention to the value\n",
    "        output = torch.bmm(attn, value)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, hid_dim, key_dim, value_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_head = num_head\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        \n",
    "#         Mapping to the desired dimension to ensure compatiability\n",
    "        self.w_qs = nn.Linear(hid_dim, num_head * key_dim)\n",
    "        self.w_ks = nn.Linear(hid_dim, num_head * key_dim)\n",
    "        self.w_vs = nn.Linear(hid_dim, num_head * value_dim)\n",
    "        \n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (hid_dim + key_dim)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (hid_dim + key_dim)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (hid_dim + value_dim)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(sqrt_dim=np.power(key_dim, 0.5))\n",
    "#         Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.fc = nn.Linear(num_head * value_dim, hid_dim)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.key_dim, self.value_dim, self.num_head\n",
    "\n",
    "#         Batch size, length of query, key and value\n",
    "        sz_b, len_q, _ = query.size()\n",
    "        sz_b, len_k, _ = key.size()\n",
    "        sz_b, len_v, _ = value.size()\n",
    "\n",
    "        residual = query\n",
    "\n",
    "        q = self.w_qs(query).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(key).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(value).view(sz_b, len_v, n_head, d_v)\n",
    "        \n",
    "#         Swithch the head to the front\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
    "\n",
    "#         Weighted combination (attention)\n",
    "        mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "        \n",
    "#         Reshape \n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "#         pdb.set_trace()\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "# The simple 2 layer feed forward layer on top of self attention layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "#         Fully connected layer 1\n",
    "        self.w_1 = nn.Conv1d(d_in, d_hid, 1) \n",
    "#         Fully connected layer 2\n",
    "        self.w_2 = nn.Conv1d(d_hid, d_in, 1) \n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base layer for the full encoder (In original paper, 6 layers of this)\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, d_inner, num_head, key_dim, value_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            num_head, hid_dim, key_dim, value_dim, dropout=dropout)\n",
    "        self.pos_ffn = FeedForward(hid_dim, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, enc_slf_attn\n",
    "\n",
    "# Base layer for the full decoder 2*self_attention+feed_forward(In original paper, 6 layers of this)\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = FeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        dec_output *= non_pad_mask\n",
    "\n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output *= non_pad_mask\n",
    "\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        dec_output *= non_pad_mask\n",
    "\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mask for the one are not mask\n",
    "def get_non_pad_mask(seq):\n",
    "    return seq.ne(PAD_idx).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# Get the the position encoding table\n",
    "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n",
    "    \n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        # zero vector for padding dimension\n",
    "        sinusoid_table[padding_idx] = 0.\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    key_sequence = seq_k\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(PAD_idx)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = torch.triu(\n",
    "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "    return subsequent_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_src_vocab, len_max_seq, d_word_vec,\n",
    "            src_embedding,\n",
    "            n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        n_position = len_max_seq + 1\n",
    "\n",
    "        self.src_word_emb = nn.Embedding.from_pretrained(torch.from_numpy(src_embedding), freeze=True)\n",
    "\n",
    "        self.position_enc = nn.Embedding.from_pretrained(\n",
    "            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n",
    "            freeze=True)\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)\n",
    "\n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq).float() + self.position_enc(src_pos).float()\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list += [enc_slf_attn]\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_tgt_vocab, len_max_seq, d_word_vec,\n",
    "            tgt_embedding,\n",
    "            n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        n_position = len_max_seq + 1\n",
    "\n",
    "        self.tgt_word_emb = nn.Embedding.from_pretrained(torch.from_numpy(tgt_embedding), freeze=True)\n",
    "\n",
    "        self.position_enc = nn.Embedding.from_pretrained(\n",
    "            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n",
    "            freeze=True)\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(tgt_seq)\n",
    "\n",
    "        slf_attn_mask_subseq = get_subsequent_mask(tgt_seq)\n",
    "        slf_attn_mask_keypad = get_attn_key_pad_mask(seq_k=tgt_seq, seq_q=tgt_seq)\n",
    "        slf_attn_mask = (slf_attn_mask_keypad + slf_attn_mask_subseq).gt(0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=tgt_seq)\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.tgt_word_emb(tgt_seq).float() + self.position_enc(tgt_pos).float()\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask,\n",
    "                dec_enc_attn_mask=dec_enc_attn_mask)\n",
    "\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list += [dec_slf_attn]\n",
    "                dec_enc_attn_list += [dec_enc_attn]\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_src_vocab, n_tgt_vocab, len_max_seq, \n",
    "            src_embedding, tgt_embedding,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1,\n",
    "            tgt_emb_prj_weight_sharing=True,\n",
    "            emb_src_tgt_weight_sharing=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, len_max_seq=len_max_seq, src_embedding = src_embedding,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_tgt_vocab=n_tgt_vocab, len_max_seq=len_max_seq, tgt_embedding = tgt_embedding,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        if tgt_emb_prj_weight_sharing:\n",
    "            # Share the weight matrix between target word embedding & the final logit dense layer\n",
    "            self.tgt_word_prj.weight = self.decoder.tgt_word_emb.weight\n",
    "            self.x_logit_scale = (d_model ** -0.5)\n",
    "        else:\n",
    "            self.x_logit_scale = 1.\n",
    "\n",
    "        if emb_src_tgt_weight_sharing:\n",
    "            # Share the weight matrix between source & target word embeddings\n",
    "            assert n_src_vocab == n_tgt_vocab, \\\n",
    "            \"To share word embedding table, the vocabulary size of src/tgt shall be the same.\"\n",
    "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "    def forward(self, src_seq, src_pos, tgt_seq, tgt_pos):\n",
    "\n",
    "        tgt_seq, tgt_pos = tgt_seq[:, :-1], tgt_pos[:, :-1]\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        seq_logit = self.tgt_word_prj(dec_output) * self.x_logit_scale\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)*1\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "def cal_performance(pred, gold, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(PAD_idx)\n",
    "    n_correct = pred.eq(gold)\n",
    "    n_correct = n_correct.masked_select(non_pad_mask).sum().item()\n",
    "\n",
    "    return loss, n_correct\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, smoothing):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "#     pdb.set_trace()\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(PAD_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=PAD_idx, reduction='sum')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, training_data, optimizer, device, smoothing):\n",
    "    ''' Epoch operation in training phase'''\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    n_word_total = 0\n",
    "    n_word_correct = 0\n",
    "    count = 0\n",
    "    for batch in tqdm(\n",
    "            training_data, mininterval=2,\n",
    "            desc='  - (Training)   ', leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq, src_pos, tgt_seq, tgt_pos = map(lambda x: x.to(device), batch)\n",
    "        gold = tgt_seq[:, 1:]\n",
    "        \n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(src_seq, src_pos, tgt_seq, tgt_pos)\n",
    "        \n",
    "        # backward\n",
    "        loss, n_correct = cal_performance(pred, gold, smoothing=smoothing)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        non_pad_mask = gold.ne(PAD_idx)\n",
    "        n_word = non_pad_mask.sum().item()\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        \n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def eval_epoch(model, validation_data, device):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    n_word_total = 0\n",
    "    n_word_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(\n",
    "                validation_data, mininterval=2,\n",
    "                desc='  - (Validation) ', leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq, src_pos, tgt_seq, tgt_pos = map(lambda x: x.to(device), batch)\n",
    "            gold = tgt_seq[:, 1:]\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, src_pos, tgt_seq, tgt_pos)\n",
    "            loss, n_correct = cal_performance(pred, gold, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            non_pad_mask = gold.ne(PAD_idx)\n",
    "            n_word = non_pad_mask.sum().item()\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, EPOCH):\n",
    "    ''' Start training '''\n",
    "\n",
    "    log_train_file = None\n",
    "    log_valid_file = None\n",
    "\n",
    "    if LOG:\n",
    "        log_train_file = LOG + '.train.log'\n",
    "        log_valid_file = LOG + '.valid.log'\n",
    "\n",
    "        print('[Info] Training performance will be written to file: {} and {}'.format(\n",
    "            log_train_file, log_valid_file))\n",
    "        \n",
    "\n",
    "        with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "            log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "            log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "\n",
    "    valid_accus = []\n",
    "    for epoch_i in range(EPOCH):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model, training_data, optimizer, device, smoothing=label_smoothing)\n",
    "        print('  - (Training)   ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  ppl=math.exp(min(train_loss, 100)), accu=100*train_accu,\n",
    "                  elapse=(time.time()-start)/60))\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(model, validation_data, device)\n",
    "        print('  - (Validation) ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "                'elapse: {elapse:3.3f} min'.format(\n",
    "                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu,\n",
    "                    elapse=(time.time()-start)/60))\n",
    "\n",
    "        valid_accus += [valid_accu]\n",
    "\n",
    "        model_state_dict = model.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "#             'settings': opt,\n",
    "            'settings': None,   \n",
    "            'epoch': epoch_i}\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            if SAVE_MODEL == 'all':\n",
    "                model_name = './chkpt_Dec14/'+SAVE_MODEL + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "                torch.save(checkpoint, model_name)\n",
    "            elif SAVE_MODEL == 'best':\n",
    "                model_name = './chkpt_Dec14/'+SAVE_MODEL + '6_layer.chkpt'\n",
    "                if valid_accu >= max(valid_accus):\n",
    "                    torch.save(checkpoint, model_name)\n",
    "                    print('    - [Info] The checkpoint file has been updated.')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if log_train_file and log_valid_file:\n",
    "            with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "                log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=train_loss,\n",
    "                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n",
    "                log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=valid_loss,\n",
    "                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam():\n",
    "    ''' Beam search '''\n",
    "\n",
    "    def __init__(self, size, device=False):\n",
    "\n",
    "        self.size = size\n",
    "        self._done = False\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.zeros((size,), dtype=torch.float, device=device)\n",
    "        self.all_scores = []\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prev_ks = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.next_ys = [torch.full((size,), PAD_idx, dtype=torch.long, device=device)]\n",
    "        self.next_ys[0][0] = SOS_idx\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        return self.get_tentative_hypothesis()\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prev_ks[-1]\n",
    "\n",
    "    @property\n",
    "    def done(self):\n",
    "        return self._done\n",
    "\n",
    "    def advance(self, word_prob):\n",
    "        \"Update beam status and check if finished or not.\"\n",
    "        num_words = word_prob.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prev_ks) > 0:\n",
    "            beam_lk = word_prob + self.scores.unsqueeze(1).expand_as(word_prob)\n",
    "        else:\n",
    "            beam_lk = word_prob[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 1st sort\n",
    "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 2nd sort\n",
    "\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "\n",
    "        # bestScoresId is flattened as a (beam x word) array,\n",
    "        # so we need to calculate which word and beam each score came from\n",
    "        prev_k = best_scores_id / num_words\n",
    "        self.prev_ks.append(prev_k)\n",
    "        self.next_ys.append(best_scores_id - prev_k * num_words)\n",
    "\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.next_ys[-1][0].item() == EOS_idx:\n",
    "            self._done = True\n",
    "            self.all_scores.append(self.scores)\n",
    "\n",
    "        return self._done\n",
    "\n",
    "    def sort_scores(self):\n",
    "        \"Sort the scores.\"\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    def get_the_best_score_and_idx(self):\n",
    "        \"Get the score of the best in the beam.\"\n",
    "        scores, ids = self.sort_scores()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    def get_tentative_hypothesis(self):\n",
    "        \"Get the decoded sequence for the current timestep.\"\n",
    "\n",
    "        if len(self.next_ys) == 1:\n",
    "            dec_seq = self.next_ys[0].unsqueeze(1)\n",
    "        else:\n",
    "            _, keys = self.sort_scores()\n",
    "            hyps = [self.get_hypothesis(k) for k in keys]\n",
    "            hyps = [[SOS_idx] + h for h in hyps]\n",
    "            dec_seq = torch.LongTensor(hyps)\n",
    "\n",
    "        return dec_seq\n",
    "\n",
    "    def get_hypothesis(self, k):\n",
    "        \"\"\" Walk back to construct the full hypothesis. \"\"\"\n",
    "        hyp = []\n",
    "        for j in range(len(self.prev_ks) - 1, -1, -1):\n",
    "            hyp.append(self.next_ys[j+1][k])\n",
    "            k = self.prev_ks[j][k]\n",
    "\n",
    "        return list(map(lambda x: x.item(), hyp[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(object):\n",
    "# The abstract for testing the function, implemented the beam search inside.\n",
    "    def __init__(self, model_path, beam_size, n_best, device):\n",
    "#         self.opt = opt\n",
    "        self.beam_size = beam_size\n",
    "        self.n_best = n_best\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        model = Transformer(\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        MAX_SENTENCE_LENGTH,\n",
    "        src_embedding = data['src'].embedding_mat,\n",
    "        tgt_embedding = data['tgt'].embedding_mat,\n",
    "        tgt_emb_prj_weight_sharing=False,\n",
    "        emb_src_tgt_weight_sharing=False,\n",
    "        d_word_vec=300,\n",
    "        d_model = 300,\n",
    "        n_layers=3,\n",
    "        n_head=8,\n",
    "        dropout=0.1).to(device)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        print('[Info] Trained model state loaded.')\n",
    "\n",
    "        model.word_prob_prj = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        model = model.to(self.device)\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def translate_batch(self, src_seq, src_pos):\n",
    "        ''' Translation work in one batch '''\n",
    "\n",
    "        def get_inst_idx_to_tensor_position_map(inst_idx_list):\n",
    "            ''' Indicate the position of an instance in a tensor. '''\n",
    "            return {inst_idx: tensor_position for tensor_position, inst_idx in enumerate(inst_idx_list)}\n",
    "\n",
    "        def collect_active_part(beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm):\n",
    "            ''' Collect tensor parts associated to active instances. '''\n",
    "\n",
    "            _, *d_hs = beamed_tensor.size()\n",
    "            n_curr_active_inst = len(curr_active_inst_idx)\n",
    "            new_shape = (n_curr_active_inst * n_bm, *d_hs)\n",
    "\n",
    "            beamed_tensor = beamed_tensor.view(n_prev_active_inst, -1)\n",
    "            beamed_tensor = beamed_tensor.index_select(0, curr_active_inst_idx)\n",
    "            beamed_tensor = beamed_tensor.view(*new_shape)\n",
    "\n",
    "            return beamed_tensor\n",
    "\n",
    "        def collate_active_info(\n",
    "                src_seq, src_enc, inst_idx_to_position_map, active_inst_idx_list):\n",
    "            # Sentences which are still active are collected,\n",
    "            # so the decoder will not run on completed sentences.\n",
    "            n_prev_active_inst = len(inst_idx_to_position_map)\n",
    "            active_inst_idx = [inst_idx_to_position_map[k] for k in active_inst_idx_list]\n",
    "            active_inst_idx = torch.LongTensor(active_inst_idx).to(self.device)\n",
    "\n",
    "            active_src_seq = collect_active_part(src_seq, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "            active_src_enc = collect_active_part(src_enc, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "            active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "\n",
    "            return active_src_seq, active_src_enc, active_inst_idx_to_position_map\n",
    "\n",
    "        def beam_decode_step(\n",
    "                inst_dec_beams, len_dec_seq, src_seq, enc_output, inst_idx_to_position_map, n_bm):\n",
    "            ''' Decode and update beam status, and then return active beam idx '''\n",
    "\n",
    "            def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n",
    "                dec_partial_seq = [b.get_current_state() for b in inst_dec_beams if not b.done]\n",
    "                dec_partial_seq = torch.stack(dec_partial_seq).to(self.device)\n",
    "                dec_partial_seq = dec_partial_seq.view(-1, len_dec_seq)\n",
    "                return dec_partial_seq\n",
    "\n",
    "            def prepare_beam_dec_pos(len_dec_seq, n_active_inst, n_bm):\n",
    "                dec_partial_pos = torch.arange(1, len_dec_seq + 1, dtype=torch.long, device=self.device)\n",
    "                dec_partial_pos = dec_partial_pos.unsqueeze(0).repeat(n_active_inst * n_bm, 1)\n",
    "                return dec_partial_pos\n",
    "\n",
    "            def predict_word(dec_seq, dec_pos, src_seq, enc_output, n_active_inst, n_bm):\n",
    "                dec_output, *_ = self.model.decoder(dec_seq, dec_pos, src_seq, enc_output)\n",
    "                dec_output = dec_output[:, -1, :]  # Pick the last step: (bh * bm) * d_h\n",
    "                word_prob = F.log_softmax(self.model.tgt_word_prj(dec_output), dim=1)\n",
    "                word_prob = word_prob.view(n_active_inst, n_bm, -1)\n",
    "\n",
    "                return word_prob\n",
    "\n",
    "            def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):\n",
    "                active_inst_idx_list = []\n",
    "                for inst_idx, inst_position in inst_idx_to_position_map.items():\n",
    "                    is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position])\n",
    "                    if not is_inst_complete:\n",
    "                        active_inst_idx_list += [inst_idx]\n",
    "\n",
    "                return active_inst_idx_list\n",
    "\n",
    "            n_active_inst = len(inst_idx_to_position_map)\n",
    "\n",
    "            dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n",
    "            dec_pos = prepare_beam_dec_pos(len_dec_seq, n_active_inst, n_bm)\n",
    "            word_prob = predict_word(dec_seq, dec_pos, src_seq, enc_output, n_active_inst, n_bm)\n",
    "\n",
    "            # Update the beam with predicted word prob information and collect incomplete instances\n",
    "            active_inst_idx_list = collect_active_inst_idx_list(\n",
    "                inst_dec_beams, word_prob, inst_idx_to_position_map)\n",
    "\n",
    "            return active_inst_idx_list\n",
    "\n",
    "        def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n",
    "            all_hyp, all_scores = [], []\n",
    "            for inst_idx in range(len(inst_dec_beams)):\n",
    "                scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n",
    "                all_scores += [scores[:n_best]]\n",
    "\n",
    "                hyps = [inst_dec_beams[inst_idx].get_hypothesis(i) for i in tail_idxs[:n_best]]\n",
    "                all_hyp += [hyps]\n",
    "            return all_hyp, all_scores\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #-- Encode\n",
    "            src_seq, src_pos = src_seq.to(self.device), src_pos.to(self.device)\n",
    "            src_enc, *_ = self.model.encoder(src_seq, src_pos)\n",
    "\n",
    "            #-- Repeat data for beam search\n",
    "            n_bm = self.beam_size\n",
    "            n_inst, len_s, d_h = src_enc.size()\n",
    "            src_seq = src_seq.repeat(1, n_bm).view(n_inst * n_bm, len_s)\n",
    "            src_enc = src_enc.repeat(1, n_bm, 1).view(n_inst * n_bm, len_s, d_h)\n",
    "\n",
    "            #-- Prepare beams\n",
    "            inst_dec_beams = [Beam(n_bm, device=self.device) for _ in range(n_inst)]\n",
    "\n",
    "            #-- Bookkeeping for active or not\n",
    "            active_inst_idx_list = list(range(n_inst))\n",
    "            inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "\n",
    "            #-- Decode\n",
    "            for len_dec_seq in range(1, MAX_SENTENCE_LENGTH + 1):\n",
    "\n",
    "                active_inst_idx_list = beam_decode_step(\n",
    "                    inst_dec_beams, len_dec_seq, src_seq, src_enc, inst_idx_to_position_map, n_bm)\n",
    "\n",
    "                if not active_inst_idx_list:\n",
    "                    break  # all instances have finished their path to <EOS>\n",
    "\n",
    "                src_seq, src_enc, inst_idx_to_position_map = collate_active_info(\n",
    "                    src_seq, src_enc, inst_idx_to_position_map, active_inst_idx_list)\n",
    "\n",
    "        batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, self.n_best)\n",
    "\n",
    "        return batch_hyp, batch_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  - (Training)   :   0%|          | 0/2084 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Trained model state loaded.\n",
      "[Info] Training performance will be written to file: Trial_Dec14_2.train.log and Trial_Dec14_2.valid.log\n",
      "[ Epoch 0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Validation) :   0%|          | 0/20 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Training)   ppl:  108.04744, accuracy: 30.037 %, elapse: 18.662 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Validation) ppl:  59.17836, accuracy: 29.008 %, elapse: 0.063 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  - (Training)   :   0%|          | 0/2084 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 1 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Validation) :   0%|          | 0/20 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Training)   ppl:  121.90687, accuracy: 28.491 %, elapse: 18.583 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :   0%|          | 0/2084 [00:00<?, ?it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Validation) ppl:  67.86104, accuracy: 26.959 %, elapse: 0.064 min\n",
      "[ Epoch 2 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :  42%|     | 884/2084 [07:53<10:42,  1.87it/s]"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "transformer = Transformer(\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        MAX_SENTENCE_LENGTH,\n",
    "        src_embedding = data['src'].embedding_mat,\n",
    "        tgt_embedding = data['tgt'].embedding_mat,\n",
    "        tgt_emb_prj_weight_sharing=False,\n",
    "        emb_src_tgt_weight_sharing=False,\n",
    "        d_word_vec=300,\n",
    "        d_model = 300,\n",
    "        n_layers=6,\n",
    "        n_head=8,\n",
    "        dropout=0.1).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"./chkpt_Dec14/best6_layer.chkpt\")\n",
    "transformer.load_state_dict(checkpoint['model'])\n",
    "print('[Info] Trained model state loaded.')\n",
    "\n",
    "optimizer = ScheduledOptim(\n",
    "        optim.Adam(\n",
    "            filter(lambda x: x.requires_grad, transformer.parameters()),\n",
    "            betas=(0.9, 0.98), eps=1e-09),\n",
    "        300, 4000)\n",
    "\n",
    "train(transformer, train_loader, val_loader, optimizer, device, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sacrebleu import raw_corpus_bleu\n",
    "from sacrebleu import corpus_bleu\n",
    "import pdb\n",
    "\n",
    "def compute_bleu(translator_path):\n",
    "    pred_sent = []\n",
    "    ref_sent = []\n",
    "\n",
    "    translator = Translator(translator_path, 1, 1, device)\n",
    "#     translator.eval()\n",
    "    for batch in tqdm(test_loader, mininterval=2, desc='  - (Test)', leave=False):\n",
    "        src_seq, src_pos, tgt_seq, tgt_pos = batch\n",
    "        all_hyp, all_scores = translator.translate_batch(src_seq, src_pos)\n",
    "    #     pdb.set_trace()\n",
    "        ref_sent+=[[test_loader.dataset.tgt_idx2word[index] for index in sent] for sent in tgt_seq.numpy()]\n",
    "\n",
    "        for idx_seqs in all_hyp:\n",
    "            for idx_seq in idx_seqs:\n",
    "                pred_line = ' '.join([test_loader.dataset.tgt_idx2word[idx] for idx in idx_seq])\n",
    "                pred_sent.append(pred_line)\n",
    "#         break\n",
    "#     pdb.set_trace()\n",
    "    _ref_sent = [[pad_parser(i) for i in ref_sent]]\n",
    "    _pred_sent = [pad_parser(i.split(' ')) for i in pred_sent]\n",
    "    print (\"Corpus bleu:\", corpus_bleu(_pred_sent, _ref_sent).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_parser(word_sequence):\n",
    "    '''\n",
    "    Remove the pad index after the eos index\n",
    "    :param word_sequence: list of the predicted words\n",
    "    :return: list of predicted words before <eos>\n",
    "    '''\n",
    "    eos = [i for i,word in enumerate(word_sequence) if word=='.' or word=='?' or word=='<EOS>']\n",
    "    word_sequence = word_sequence[:eos[-1]] if len(eos)!= 0 else word_sequence\n",
    "    return ' '.join(word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  - (Test):   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Trained model state loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-59-c92daa3fc3c6>(23)compute_bleu()\n",
      "-> _ref_sent = [[pad_parser(i) for i in ref_sent]]\n",
      "(Pdb) c\n",
      "Corpus bleu: 4.3963106233489455\n"
     ]
    }
   ],
   "source": [
    "compute_bleu('./chkpt_Dec14/best.chkpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
